# Beta Testing

To validate and refine the processing workflow for the **GLC_FCS30D dataset**, we begin with a **single beta-test folder**. Each folder represents one large extent (e.g., `GLCFC30D_19852022maps_E140-E145`) and contains approximately **34 sub-tiles**. Each sub-tile is a multi-band GeoTIFF with **23 annual layers** spanning **2000â€“2022**.  

The goal of this beta test is to design and validate the pipeline steps on a constrained subset of the data before scaling to all 36 global extents.

---

## General Considerations:

#### Structure of the GLC_FCS30D Raw Data

The **GLC_FCS30D dataset** is distributed as **36 large tiles**, each provided as a compressed ZIP archive ranging in size from **1â€“8 GB**.  

- **Large tile coverage**: each ZIP archive spans a **5Â° Ã— 5Â° longitudeâ€“latitude block** (e.g., `E140â€“E145`).  
- **Sub-tiles inside each ZIP**: within each archive are **34 smaller GeoTIFF tiles**, each representing a **5Â° Ã— 5Â° subregion** partitioned across latitude and longitude (e.g., `E140N0` through `E145S40`). Together, these sub-tiles fill the full spatial extent of the larger ZIPâ€™s geographic coverage.  
- **Temporal structure**: each sub-tile is a **multi-band GeoTIFF**, where each band corresponds to one year of land cover data spanning **2000â€“2022**. This results in **23 annual layers** per sub-tile.  

In summary, the dataset provides consistent global land cover coverage at 30 m resolution, organized hierarchically by **large geographic blocks â†’ sub-tiles â†’ annual layers**.

---

#### Challenges from Raw Data Structure

The challenge lies in translating the **GLC_FCS30D raw data structure** into an efficient, programmatic process that can scale globally.  

While similar coding has been successfully implemented for the **HILDE+ dataset**â€”a land cover dataset at a coarser **1 km resolution**â€”the preliminary functions used for HILDE+ (those preceding the computation of proportionality of land cover classifications within individual PRIO-GRID cells) do not translate directly to GLC.  

The reason is structural:  
- **HILDE+**: Each year is already separated into its own GeoTIFF, and each file represents a single global tile. This makes direct yearly processing straightforward.  
- **GLC_FCS30D**: Data are nested in a more complex hierarchy. A primary ZIP file contains ~35 sub-tiles, and each sub-tile is a multi-band GeoTIFF with 23 annual layers. This requires two levels of unpacking and relational mapping before analysis can begin.  

This represents a **two-order deficit in translation** between the two methodsâ€”making it infeasible to immediately apply prefabricated HILDE+ functions to GLC without significant restructuring.  

---

#### PRIO-GRID overlay assumptions
- Each sub-tile is expected to map approximately to a **10 Ã— 10 grid of PRIO-GRID (PG) cells**, still with 23 year layers.
- This defines the spatial resolution bridge between the 30 m landcover data and the ~55 km PG grid.

- `robust_transformation` utility: Resamples to PRIO-GRID resolution â†’ ensures the output aligns to whole-degree increments (1Â°, 0.5Â°, 0.25Â°, 0.2Â°, etc.), which makes aggregation mathematically consistent across all tiles.

  - *res <- robust_transformation(res, agg_fun = "mean")*

---

#### Filtering year range
- The user specifies a date range via **`pgoptions`**.  
- For the beta test, we simplify to **2015â€“2020**.  

**Action step**:  
- Implement a function (possibly adapted from HILDE or CRU code) to **rename raster layers by year** (2000â€“2022).  
- Then, filter from the `annual` field in `pgoptions` to retain only the desired subset of years.  
- Result: a 6-year stack (2015â€“2020) per sub-tile.

---

#### Reclassify landcover categories
- Each sub-tile contains multiple landcover classes.  
- To simplify, we will use a **classification dictionary** to merge many classes into fewer categories.  

**Action step**:  
- Develop a default **landcover dictionary** (e.g., forest, cropland, urban, water).  
- Keep the dictionary as a **parameter** so alternative mappings can be tested flexibly.

---

#### Proportional extraction by PRIO-GRID cell
For each selected year in the sub-tile, apply a function to calculate:  

1. **PRIO-GRID ID**: assign `pg_id` for each cell intersecting the sub-tile.  
2. **Proportion of PG cell covered**:  
   - Each PG cell may overlap partially with the sub-tile extent.  
   - Compute the percentage coverage per PG cell, expressed as a **confidence field**.  
   - Anticipated coverage: typically >95%, but lower values possible along boundaries.  
3. **Proportional landcover share**: the fraction of each PG cell occupied by each landcover type.  
4. **Parent file reference**: derive from the sub-tile filename (e.g., `GLC_FC30D_20002022_E140N0_Annual.tif` â†’ `parent_file = "E140N0"`).  
5. **Grandparent folder reference**: derive from the enclosing ZIP/folder name (e.g., `GLCFC30D_19852022maps_E140-E145` â†’ `grandparent_folder = "E140-E145"`).

---

#### Dataframe output
The resulting dataframe represents **one year of one landcover type from one sub-tile**. It includes:  

- `pg_id` â€” PRIO-GRID cell ID  
- `year` â€” year of data (filtered to 2015â€“2020 in this test)  
- `landcover_type` â€” class after dictionary reclassification  
- `p_lc_area` â€” proportional area share of the PG cell covered by the landcover type  
- `confidence` â€” coverage proportion of PG cell relative to the sub-tile  
- `parent_file` â€” sub-tile code (e.g., `E140N0`)  
- `grandparent_folder` â€” parent folder extent (e.g., `E140-E145`)  

---

## Operational Tasks 


### Hierarchy of Loops

The workflow for processing **GLC_FCS30D** landcover data is structured in **four stages** and **three looping levels**:  

- **Stage One: Data download**  
  Functions used: `zen_filelist`, `split_by_extension`, and `zen_download`.  
  > *Note: this may present unique chokepoints for the standard **ingestion and documentation** process but reflects agreed-upon tasking from @andreas in a meeting Friday (26 Sept). This documentation presents transparency to @Jonas in approving the *non-standardised* procedure, implemented to conserve download time and maintain efficiency in beta testing these GLC landcover variables for PSI.*  

- **First Level:**  
  Following the download to disk, a zipped folder contains 1 of 36 subglobal extents. The first level of looping operations concentrates on the sub-tile extent of an individual file contained within the zipped folder â€” what we define as the *primary extent*.  

- **Second Level:**  
  Operations that consolidate a data feature to the *primary extent*, this level of analysis.

- **Third Level (Global extent):**  
  Operations for levels 1 and 2 are repeated across all folders (primary extents) that are available or retrieved from the Stage One download operation, reaching the global extent of PRIO-GRID.  

---

### Tasks Performed Within Looping Heirarchy

**Ingestion Level Preprocessing Tasks**
#### A: Construct `read` function

**First Level Tasks**

#### Step 1. Process all years of interest
- Input: one sub-tile with 23 layers (2000â€“2022).  
- Filter to `pgoptions` annual range (e.g., 2015â€“2020).  
- Rename bands to match years.  
- Output: stack of selected annual layers.  

#### Step 2. Reclassify landcover
- Apply dictionary mapping 30+ native classes â†’ reduced set (e.g., 6 categories).  
- Keep dictionary flexible (parameter).  
- Output: reclassified raster stack.  

#### Step 3. Overlay PRIO-GRID
- Intersect sub-tile extent with PG polygons (~10Ã—10 cells).  
- For each PG cell and each year:  
  - calculate proportional coverage (â†’ `confidence`)  
  - calculate proportional area per landcover type (â†’ `p_lc_area`).  

#### Step 4. Add metadata
- Attach `pg_id`, `year`, `landcover_type`, `parent_file`, `grandparent_folder`.  
- Output: tidy dataframe (long format).  

#### Step 5. Write incrementally
- Append dataframe to disk in **Parquet** or **Feather** format.  
- Use `arrow::write_dataset()` or `arrow::write_parquet()` for efficient appending.  
- After writing, clear memory.  

#### Step 6. Repeat across sub-tiles
- Move to next sub-tile.  
- Run Steps 1â€“5.  
- Append results into the same dataset file.  

#### Step 7. Complete folder extent
- After 34 sub-tiles are processed, the result is a **single dataset covering the folder extent** (e.g., `E140â€“E145`).  
- Dataset is tidy long format: all years Ã— all landcover classes Ã— all PG cells for that extent.  

#### Step 8. Scale to global
- Repeat Steps 1â€“7 for all 36 global ZIP folders.  
- Final output: unified dataset (Parquet/Feather) covering all extents.  
- Supports efficient queries by `pg_id`, `year`, or `landcover_type`.  

```{r}


#' Reads the GLC v2 dataset
#'
#'
#'
#' @return an object of class sf
#' @export
#'
#' @references
#' \insertRef{winklerHILDAGlobalLand2020}{priogrid}
read_glc_v2 <- function(beta_test = FALSE) {
  # Base dataset directory
  f <- file.path(
    "/Volumes/T7/PRIOGRID",
    "GLC_FCS30",
    "v2",
    "7f03a296-4329-4458-8b62-83c3d27530af"
  )
  
  # ---- Identify zip files ----
  zips <- list.files(f, pattern = "\\.zip$", full.names = TRUE)
  
  # Optional beta test: only process first 2 zips
  if (beta_test) {
    message("Running in beta test mode: limiting to first two zip files.")
    zips <- head(zips, 2)
  }
  
  # ---- Unzip only if not already unzipped ----
  unzipped_dirs <- vapply(zips, function(z) {
    target_dir <- sub("\\.zip$", "", z)
    if (!dir.exists(target_dir)) {
      message("Unzipping ", basename(z))
      unzip(z, exdir = target_dir)
    } else {
      message("Skipping (already unzipped): ", basename(z))
    }
    return(target_dir)
  }, FUN.VALUE = character(1))
  
  # ---- Gather all annual (not 5-year) TIFFs ----
  tif_files <- list.files(
    unzipped_dirs,
    pattern = "\\.tif$",
    full.names = TRUE,
    recursive = TRUE
  )
  tif_files <- tif_files[!grepl("5years", basename(tif_files), ignore.case = TRUE)]
  
  return(tif_files)
  
  # ---- Stack lazily ----
  # r <- terra::rast(tif_files)
}


```


```{r}
tif_files = read_glc_v2(beta_test = TRUE)
```

Next incorporate:
```{r}
library(terra)

```

```{r}
tif_files
```
```{r}
prepare_glc_layers <- function(tif_files) {
  library(terra)
  library(lubridate)

  # ---- Step 1: Load and rename rasters by year range ----
  ras_list <- vector("list", length(tif_files))
  
  for (i in seq_along(tif_files)) {
    r_i <- rast(tif_files[i])
    fname <- basename(tif_files[i])
    
    # Parse start and end years from filename
    start_year <- as.numeric(sub(".*_(\\d{4})\\d{4}.*", "\\1", fname))
    end_year   <- as.numeric(sub(".*_(\\d{8}).*", "\\1", fname))
    end_year   <- as.numeric(substr(end_year, 5, 8))
    
    # Build the sequence of years
    years <- seq(start_year, end_year)
    
    # Rename layers safely
    names(r_i) <- years[seq_len(min(length(years), nlyr(r_i)))]
    
    ras_list[[i]] <- r_i
  }

  # ---- Step 2: Filter layers by PRIO-GRID years ----
  years_to_keep <- year(pg_dates()) |> unique()
  
  ras_list_filtered <- lapply(ras_list, function(r) {
    raw_names <- names(r)
    layer_years <- suppressWarnings(as.numeric(gsub("\\D", "", raw_names)))
    keep_layers <- layer_years %in% years_to_keep
    
    if (any(keep_layers)) {
      r <- r[[keep_layers]]
      names(r) <- layer_years[keep_layers]
    } else {
      warning("No matching years found for raster: ", sources(r))
      r <- NULL
    }
    return(r)
  })
  
  # Remove NULL rasters
  ras_list_filtered <- Filter(Negate(is.null), ras_list_filtered)
  
  # ---- Step 3: Rename to full YYYY-MM-DD format ----
  month_to_use <- pg_dates() |> month() |> max()
  day_to_use   <- pg_dates() |> day()   |> max()
  
  ras_list_filtered <- lapply(ras_list_filtered, function(r) {
    yrs <- as.numeric(names(r))
    names(r) <- as.Date(paste(yrs, month_to_use, day_to_use, sep = "-"))
    return(r)
  })
  
  # ---- Return processed rasters ----
  message("Layer preparation complete. Returning filtered raster list.")
  return(ras_list_filtered)
}

```


```{r}
ras_list_filtered <- prepare_glc_layers(tif_files)
```

```{r}
ras_list_filtered
```

```{r}
# Step 3: Verify results
# --------------------------------------

# 1ï¸âƒ£ Check how many raster tiles were returned
length(ras_list_filtered)

# 2ï¸âƒ£ Check the number of layers per raster (should match pg_dates() years)
sapply(ras_list_filtered, nlyr)

# 3ï¸âƒ£ Inspect the first raster's layer names
head(names(ras_list_filtered[[1]]))

# 4ï¸âƒ£ Inspect the full date range for the first raster
range(names(ras_list_filtered[[1]]))

# 5ï¸âƒ£ Optional â€” quick visual preview of one raster layer
plot(ras_list_filtered[[1]][[1]], main = names(ras_list_filtered[[1]])[1])
```
```{r}
class(ras_list_filtered)
class(ras_list_filtered[[1]])

# Inspect the CRS, extent, and resolution of your rasters
crs_list <- sapply(ras_list_filtered, function(r) as.character(terra::crs(r)))
ext_list <- lapply(ras_list_filtered, terra::ext)
res_list <- lapply(ras_list_filtered, terra::res)

# Check CRS consistency
unique(crs_list)

# Compare resolutions
unique(do.call(rbind, res_list))

# Compare extents
ext_summary <- do.call(rbind, lapply(ext_list, function(e) as.numeric(e)))
colnames(ext_summary) <- c("xmin", "xmax", "ymin", "ymax")

summary(ext_summary)

```

```{r}
inspect_glc_tiles <- function(beta_test = TRUE, sample_n = NULL) {
  # Read and prepare tiles (same process as your main workflow)
  tif_files <- read_glc_v2(beta_test = beta_test)
  ras_list_filtered <- prepare_glc_layers(tif_files)

  message("Inspecting ", length(ras_list_filtered), " GLC tiles...")

  # Optionally inspect only a subset (for speed)
  if (!is.null(sample_n)) {
    ras_list_filtered <- ras_list_filtered[seq_len(min(sample_n, length(ras_list_filtered)))]
  }

  # Create a summary data frame
  stats_list <- lapply(seq_along(ras_list_filtered), function(i) {
    r <- ras_list_filtered[[i]]
    fname <- basename(sources(r))
    message("Checking tile ", i, "/", length(ras_list_filtered), ": ", fname)

    # Get unique values across all layers (quick sample if huge)
    values_sample <- tryCatch({
      terra::values(r, mat = FALSE)
    }, error = function(e) {
      message("    âš ï¸ Could not read values for ", fname, ": ", e$message)
      return(NULL)
    })

    if (is.null(values_sample)) return(NULL)

    # Flatten and summarize
    tab <- table(values_sample, useNA = "no")
    tab_df <- data.frame(
      tile = fname,
      class_value = as.integer(names(tab)),
      count = as.numeric(tab)
    )

    tab_df$total_pixels <- sum(tab_df$count)
    tab_df$prop <- tab_df$count / tab_df$total_pixels
    tab_df
  })

  # Combine results
  stats_df <- do.call(rbind, Filter(Negate(is.null), stats_list))

  message("Inspection complete.")
  return(stats_df)
}

```

```{r}
glc_summary <- inspect_glc_tiles(beta_test = TRUE, sample_n = 5)
head(glc_summary)

```
```{r}
robust_glc_landcover <- function(landcovertype, beta_test = FALSE) {
  memfrac_option <- terra::terraOptions(verbose = FALSE)$memfrac
  terra::terraOptions(memfrac = 0.8)

  # Read and prepare tiles
  tif_files <- read_glc_v2(beta_test = beta_test)
  ras_list_filtered <- prepare_glc_layers(tif_files)

  message("Running landcover computation on ", length(ras_list_filtered), " tiles.")
  message("Each tile will be processed via robust_transformation.\n")

  # Apply robust transformation to each tile
  transformed_tiles <- lapply(seq_along(ras_list_filtered), function(i) {
    tile <- ras_list_filtered[[i]]
    tile_name <- names(ras_list_filtered)[i] %||% paste0("Tile_", i)

    message(i, "/", length(ras_list_filtered), "] Processing ", tile_name)

    tryCatch({
      res_tile <- robust_transformation(
        tile,
        function(x) mean(x %in% landcovertype, na.rm = TRUE)
      )
      gc()
      res_tile
    }, error = function(e) {
      message("Skipping problematic tile: ", tile_name, " â€” ", e$message)
      NULL
    })
  })

  # Drop NULLs
  transformed_tiles <- Filter(Negate(is.null), transformed_tiles)

  # Merge results
  if (length(transformed_tiles) > 1) {
    message("\n Merging ", length(transformed_tiles), " aggregated tiles...")
    res <- do.call(terra::merge, transformed_tiles)
  } else {
    res <- transformed_tiles[[1]]
  }

  message("Landcover computation complete.")
  terra::terraOptions(memfrac = memfrac_option)
  return(res)
}

```


```{r}
glc_landcover <- function(landcovertype, beta_test = FALSE) {
  memfrac_option <- terra::terraOptions(verbose = FALSE)$memfrac
  terra::terraOptions(memfrac = 0.8)

  # 1ï¸âƒ£ Read and prepare tiles
  tif_files <- read_glc_v2(beta_test = beta_test)
  ras_list_filtered <- prepare_glc_layers(tif_files)

  message("Running landcover computation on ", length(ras_list_filtered), " tiles.")
  message("Each tile will be processed per layer to preserve temporal information.\n")

  # Ensure CRS consistency
  #ras_list_filtered <- lapply(ras_list_filtered, function(r) {
  #  if (is.na(terra::crs(r))) {
  #    terra::crs(r) <- "EPSG:4326"
  #  }
  #  r
  #}

  # 3ï¸âƒ£ Define helper for one tile
  process_tile <- function(tile, landcovertype) {
    layer_results <- lapply(1:terra::nlyr(tile), function(k) {
      lyr <- tile[[k]]
      terra::aggregate(
        lyr,
        fact = 1830, # 30 m â†’ ~55 km
        fun = function(x) mean(x %in% landcovertype, na.rm = TRUE)
      )
    })
    names(layer_results) <- names(tile)
    terra::rast(layer_results)
  }

  # 4ï¸âƒ£ Process tiles safely
  tile_results <- vector("list", length(ras_list_filtered))
  for (i in seq_along(ras_list_filtered)) {
    tile_name <- basename(sources(ras_list_filtered[[i]]))
    message("â³ [", i, "/", length(ras_list_filtered), "] Processing tile: ", tile_name)

    tile_results[[i]] <- tryCatch(
      process_tile(ras_list_filtered[[i]], landcovertype),
      error = function(e) {
        message("    âš ï¸ Skipping problematic tile: ", tile_name, " â€” ", e$message)
        NULL
      }
    )

    gc()
  }

  # 5ï¸âƒ£ Drop NULLs and merge tiles spatially
  tile_results <- Filter(Negate(is.null), tile_results)

  if (length(tile_results) > 1) {
    message("\nðŸ§© Merging ", length(tile_results), " aggregated tiles into a single spatial raster...")
    res <- do.call(terra::merge, tile_results)
  } else {
    res <- tile_results[[1]]
  }

  message("âœ… Landcover computation complete.")
  terra::terraOptions(memfrac = memfrac_option)
  return(res)
}


```


```{r}
# Load first raster tile from your GLC beta test set
r <- rast(tif_files[1])

cat("\nðŸ§­ Inspecting raster:", basename(tif_files[1]), "\n")
cat("--------------------------------------------------\n")

# 1ï¸âƒ£ Check if the raster is categorical
is_cat <- terra::is.factor(r)
cat("Is this a categorical (factor) raster?  ", is_cat, "\n")

# 2ï¸âƒ£ Check number of layers
num_layers <- terra::nlyr(r)
cat("Number of layers: ", num_layers, "\n")

# 3ï¸âƒ£ See CRS and extent (just for orientation)
cat("CRS: ", terra::crs(r), "\n")
cat("Extent: ", toString(terra::ext(r)), "\n")

# 4ï¸âƒ£ Data type (e.g., INT1U, INT2S, FLT4S, etc.)
cat("Data type: ", terra::datatype(r), "\n")

# 5ï¸âƒ£ Peek at unique values (first layer only)
cat("\nUnique values in first layer (showing first 50):\n")
print(head(terra::unique(r[[1]], na.rm = TRUE), 50))

```


```{r}
# --- Cropland ---
gen_glc_cropland <- function(beta_test = FALSE) {
  glc_landcover(
    landcovertype = c(10, 11, 12, 20),
    beta_test = beta_test
  )
}

# --- Forest ---
gen_glc_forest <- function(beta_test = FALSE) {
  glc_landcover(
    landcovertype = c(51, 52, 61, 62, 71, 72, 81, 82, 91, 92),
    beta_test = beta_test
  )
}

# --- Shrubland ---
gen_glc_shrubland <- function(beta_test = FALSE) {
  glc_landcover(
    landcovertype = c(120, 121, 122, 150, 152),
    beta_test = beta_test
  )
}

# --- Grassland ---
gen_glc_grassland <- function(beta_test = FALSE) {
  glc_landcover(
    landcovertype = c(130, 153),
    beta_test = beta_test
  )
}

# --- Wetland ---
gen_glc_wetland <- function(beta_test = FALSE) {
  glc_landcover(
    landcovertype = c(181, 182, 183, 184, 185, 186, 187),
    beta_test = beta_test
  )
}

# --- Built-up / Urban ---
gen_glc_builtup <- function(beta_test = FALSE) {
  glc_landcover(
    landcovertype = 190,
    beta_test = beta_test
  )
}
  
# --- Water Body ---
gen_glc_water <- function(beta_test = FALSE) {
  glc_landcover(
    landcovertype = 210,
    beta_test = beta_test
  )
}

```


```{r}
gen_glc_shrubland(beta_test = TRUE)
```



### Sandbox:

```{r}
# Create a list to hold renamed rasters
ras_list <- vector("list", length(tif_files))

for (i in seq_along(tif_files)) {
  # Load one tif lazily
  r_i <- rast(tif_files[i])
  fname <- basename(tif_files[i])
  
  # Parse start and end years from filename
  start_year <- as.numeric(sub(".*_(\\d{4})\\d{4}.*", "\\1", fname))
  end_year   <- as.numeric(sub(".*_(\\d{8}).*", "\\1", fname))
  end_year   <- as.numeric(substr(end_year, 5, 8))
  
  # Build the sequence of years
  years <- seq(start_year, end_year)
  
  # Rename layers safely
  names(r_i) <- years[seq_len(min(length(years), nlyr(r_i)))]
  
  # Store renamed raster in list
  ras_list[[i]] <- r_i
}

# Inspect one example
names(ras_list[[30]])[1:10]

```
## Step 2
```{r}
# Replace your previous filter step with this improved one
years_to_keep <- lubridate::year(pg_dates()) |> unique()

ras_list_filtered <- lapply(ras_list, function(r) {
  # Clean up names BEFORE filtering
  raw_names <- names(r)
  layer_years <- suppressWarnings(as.numeric(gsub("\\D", "", raw_names)))
  
  # Now filter
  keep_layers <- layer_years %in% years_to_keep
  
  if (any(keep_layers)) {
    r <- r[[keep_layers]]
    names(r) <- layer_years[keep_layers]  # overwrite with clean numeric years
  } else {
    warning("No matching years found for raster: ", sources(r))
    r <- NULL
  }
  
  return(r)
})
```

## Step 3
```{r}
# Continue with downstream steps (unchanged)
ras_list_filtered <- Filter(Negate(is.null), ras_list_filtered)

month_to_use <- pg_dates() |> lubridate::month() |> max()
day_to_use   <- pg_dates() |> lubridate::day()   |> max()

# Rename to full date labels (safe)
ras_list_filtered <- lapply(ras_list_filtered, function(r) {
  yrs <- as.numeric(names(r))
  names(r) <- as.Date(paste(yrs, month_to_use, day_to_use, sep = "-"))
  return(r)
})

```

```{r}
# Inspect the layer names from the first raster
head(names(ras_list_filtered[[1]]))

```


```{r}
library(terra)

# Path to your example tif
f <- "/Volumes/T7/PRIOGRID/GLC_FCS30/v2/7f03a296-4329-4458-8b62-83c3d27530af/GLC_FCS30D_19852022maps_E0-E5/GLC_FCS30D_20002022_E0N50_Annual_V1.1.tif"

# Load the file lazily
r <- rast(f)

# Check metadata
r

# Extract start/end years from filename
fname <- basename(f)
start_year <- as.numeric(sub(".*_(\\d{4})\\d{4}.*", "\\1", fname))
end_year   <- as.numeric(sub(".*_(\\d{8}).*", "\\1", fname))
end_year   <- as.numeric(substr(end_year, 5, 8))  # just last 4 digits

# Build sequence of years
years <- seq(start_year, end_year)

# Assign layer names as actual years
names(r) <- years

# Check first few
names(r)[1:23]

```

```{r}
f <- "/Volumes/T7/PRIOGRID/GLC_FCS30/v2/7f03a296-4329-4458-8b62-83c3d27530af/GLC_FCS30D_19852022maps_E0-E5/GLC_FCS30D_20002022_E0N50_Annual_V1.1.tif"
r <- rast(f)

# Extract start/end years from filename
fname <- basename(f)
start_year <- as.numeric(sub(".*_(\\d{4})\\d{4}.*", "\\1", fname))
end_year   <- as.numeric(sub(".*_(\\d{8}).*", "\\1", fname))
end_year   <- as.numeric(substr(end_year, 5, 8))  # just last 4 digits

# Build sequence of years
years <- seq(start_year, end_year)

# Assign layer names as actual years
names(r) <- years

# Check first few
names(r)[1:10]
```


```{r}
ras_list_filtered <- prepare_glc_layers(beta_test = TRUE,foldersize = 1, n_tifs = 2)

```

```{r}
ras_list_filtered
```


```{r}
# 1. Load the raster normally
lyr <- tile[[1]]

# 2. Check alignment between raster and PRIO-GRID
pg <- pg_raster()  # or whatever your PRIO reference raster is
terra::compareGeom(lyr, pg)

```

