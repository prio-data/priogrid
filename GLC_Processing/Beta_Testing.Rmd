# Beta Testing

To validate and refine the processing workflow for the **GLC_FCS30D dataset**, we begin with a **single beta-test folder**. Each folder represents one large extent (e.g., `GLCFC30D_19852022maps_E140-E145`) and contains approximately **34 sub-tiles**. Each sub-tile is a multi-band GeoTIFF with **23 annual layers** spanning **2000–2022**.  

The goal of this beta test is to design and validate the pipeline steps on a constrained subset of the data before scaling to all 36 global extents.

---

## General Considerations:

#### Structure of the GLC_FCS30D Raw Data

The **GLC_FCS30D dataset** is distributed as **36 large tiles**, each provided as a compressed ZIP archive ranging in size from **1–8 GB**.  

- **Large tile coverage**: each ZIP archive spans a **5° × 5° longitude–latitude block** (e.g., `E140–E145`).  
- **Sub-tiles inside each ZIP**: within each archive are **34 smaller GeoTIFF tiles**, each representing a **5° × 5° subregion** partitioned across latitude and longitude (e.g., `E140N0` through `E145S40`). Together, these sub-tiles fill the full spatial extent of the larger ZIP’s geographic coverage.  
- **Temporal structure**: each sub-tile is a **multi-band GeoTIFF**, where each band corresponds to one year of land cover data spanning **2000–2022**. This results in **23 annual layers** per sub-tile.  

In summary, the dataset provides consistent global land cover coverage at 30 m resolution, organized hierarchically by **large geographic blocks → sub-tiles → annual layers**.

---

#### Challenges from Raw Data Structure

The challenge lies in translating the **GLC_FCS30D raw data structure** into an efficient, programmatic process that can scale globally.  

While similar coding has been successfully implemented for the **HILDE+ dataset**—a land cover dataset at a coarser **1 km resolution**—the preliminary functions used for HILDE+ (those preceding the computation of proportionality of land cover classifications within individual PRIO-GRID cells) do not translate directly to GLC.  

The reason is structural:  
- **HILDE+**: Each year is already separated into its own GeoTIFF, and each file represents a single global tile. This makes direct yearly processing straightforward.  
- **GLC_FCS30D**: Data are nested in a more complex hierarchy. A primary ZIP file contains ~35 sub-tiles, and each sub-tile is a multi-band GeoTIFF with 23 annual layers. This requires two levels of unpacking and relational mapping before analysis can begin.  

This represents a **two-order deficit in translation** between the two methods—making it infeasible to immediately apply prefabricated HILDE+ functions to GLC without significant restructuring.  

---

#### PRIO-GRID overlay assumptions
- Each sub-tile is expected to map approximately to a **10 × 10 grid of PRIO-GRID (PG) cells**, still with 23 year layers.
- This defines the spatial resolution bridge between the 30 m landcover data and the ~55 km PG grid.

- `robust_transformation` utility: Resamples to PRIO-GRID resolution → ensures the output aligns to whole-degree increments (1°, 0.5°, 0.25°, 0.2°, etc.), which makes aggregation mathematically consistent across all tiles.

  - *res <- robust_transformation(res, agg_fun = "mean")*

---

#### Filtering year range
- The user specifies a date range via **`pgoptions`**.  
- For the beta test, we simplify to **2015–2020**.  

**Action step**:  
- Implement a function (possibly adapted from HILDE or CRU code) to **rename raster layers by year** (2000–2022).  
- Then, filter from the `annual` field in `pgoptions` to retain only the desired subset of years.  
- Result: a 6-year stack (2015–2020) per sub-tile.

---

#### Reclassify landcover categories
- Each sub-tile contains multiple landcover classes.  
- To simplify, we will use a **classification dictionary** to merge many classes into fewer categories.  

**Action step**:  
- Develop a default **landcover dictionary** (e.g., forest, cropland, urban, water).  
- Keep the dictionary as a **parameter** so alternative mappings can be tested flexibly.

---

#### Proportional extraction by PRIO-GRID cell
For each selected year in the sub-tile, apply a function to calculate:  

1. **PRIO-GRID ID**: assign `pg_id` for each cell intersecting the sub-tile.  
2. **Proportion of PG cell covered**:  
   - Each PG cell may overlap partially with the sub-tile extent.  
   - Compute the percentage coverage per PG cell, expressed as a **confidence field**.  
   - Anticipated coverage: typically >95%, but lower values possible along boundaries.  
3. **Proportional landcover share**: the fraction of each PG cell occupied by each landcover type.  
4. **Parent file reference**: derive from the sub-tile filename (e.g., `GLC_FC30D_20002022_E140N0_Annual.tif` → `parent_file = "E140N0"`).  
5. **Grandparent folder reference**: derive from the enclosing ZIP/folder name (e.g., `GLCFC30D_19852022maps_E140-E145` → `grandparent_folder = "E140-E145"`).

---

#### Dataframe output
The resulting dataframe represents **one year of one landcover type from one sub-tile**. It includes:  

- `pg_id` — PRIO-GRID cell ID  
- `year` — year of data (filtered to 2015–2020 in this test)  
- `landcover_type` — class after dictionary reclassification  
- `p_lc_area` — proportional area share of the PG cell covered by the landcover type  
- `confidence` — coverage proportion of PG cell relative to the sub-tile  
- `parent_file` — sub-tile code (e.g., `E140N0`)  
- `grandparent_folder` — parent folder extent (e.g., `E140-E145`)  

---

## Operational Tasks 


### Hierarchy of Loops

The workflow for processing **GLC_FCS30D** landcover data is structured in **four stages** and **three looping levels**:  

- **Stage One: Data download**  
  Functions used: `zen_filelist`, `split_by_extension`, and `zen_download`.  
  > *Note: this may present unique chokepoints for the standard **ingestion and documentation** process but reflects agreed-upon tasking from @andreas in a meeting Friday (26 Sept). This documentation presents transparency to @Jonas in approving the *non-standardised* procedure, implemented to conserve download time and maintain efficiency in beta testing these GLC landcover variables for PSI.*  

- **First Level:**  
  Following the download to disk, a zipped folder contains 1 of 36 subglobal extents. The first level of looping operations concentrates on the sub-tile extent of an individual file contained within the zipped folder — what we define as the *primary extent*.  

- **Second Level:**  
  Operations that consolidate a data feature to the *primary extent*, this level of analysis.

- **Third Level (Global extent):**  
  Operations for levels 1 and 2 are repeated across all folders (primary extents) that are available or retrieved from the Stage One download operation, reaching the global extent of PRIO-GRID.  

---

### Tasks Performed Within Looping Heirarchy

**Ingestion Level Preprocessing Tasks**
#### A: Construct `read` function

**First Level Tasks**

#### Step 1. Process all years of interest
- Input: one sub-tile with 23 layers (2000–2022).  
- Filter to `pgoptions` annual range (e.g., 2015–2020).  
- Rename bands to match years.  
- Output: stack of selected annual layers.  

#### Step 2. Reclassify landcover
- Apply dictionary mapping 30+ native classes → reduced set (e.g., 6 categories).  
- Keep dictionary flexible (parameter).  
- Output: reclassified raster stack.  

#### Step 3. Overlay PRIO-GRID
- Intersect sub-tile extent with PG polygons (~10×10 cells).  
- For each PG cell and each year:  
  - calculate proportional coverage (→ `confidence`)  
  - calculate proportional area per landcover type (→ `p_lc_area`).  

#### Step 4. Add metadata
- Attach `pg_id`, `year`, `landcover_type`, `parent_file`, `grandparent_folder`.  
- Output: tidy dataframe (long format).  

#### Step 5. Write incrementally
- Append dataframe to disk in **Parquet** or **Feather** format.  
- Use `arrow::write_dataset()` or `arrow::write_parquet()` for efficient appending.  
- After writing, clear memory.  

#### Step 6. Repeat across sub-tiles
- Move to next sub-tile.  
- Run Steps 1–5.  
- Append results into the same dataset file.  

#### Step 7. Complete folder extent
- After 34 sub-tiles are processed, the result is a **single dataset covering the folder extent** (e.g., `E140–E145`).  
- Dataset is tidy long format: all years × all landcover classes × all PG cells for that extent.  

#### Step 8. Scale to global
- Repeat Steps 1–7 for all 36 global ZIP folders.  
- Final output: unified dataset (Parquet/Feather) covering all extents.  
- Supports efficient queries by `pg_id`, `year`, or `landcover_type`.  

```{r}

#' Reads the GLC v2 dataset
#'
#'
#'
#' @return an object of class sf
#' @export
#'
#' @references
#' \insertRef{winklerHILDAGlobalLand2020}{priogrid}
read_glc_v2 <- function(beta_test = FALSE) {
  # Base dataset directory
  f <- file.path(
    "/Volumes/T7/PRIOGRID",
    "GLC_FCS30",
    "v2",
    "7f03a296-4329-4458-8b62-83c3d27530af"
  )
  
  # ---- Identify zip files ----
  zips <- list.files(f, pattern = "\\.zip$", full.names = TRUE)
  
  # Optional beta test: only process first 2 zips
  if (beta_test) {
    message("Running in beta test mode: limiting to first two zip files.")
    zips <- head(zips, 2)
  }
  
  # ---- Unzip only if not already unzipped ----
  unzipped_dirs <- vapply(zips, function(z) {
    target_dir <- sub("\\.zip$", "", z)
    if (!dir.exists(target_dir)) {
      message("Unzipping ", basename(z))
      unzip(z, exdir = target_dir)
    } else {
      message("Skipping (already unzipped): ", basename(z))
    }
    return(target_dir)
  }, FUN.VALUE = character(1))
  
  # ---- Gather all annual (not 5-year) TIFFs ----
  tif_files <- list.files(
    unzipped_dirs,
    pattern = "\\.tif$",
    full.names = TRUE,
    recursive = TRUE
  )
  tif_files <- tif_files[!grepl("5years", basename(tif_files), ignore.case = TRUE)]
  
  return(tif_files)
  
  # ---- Stack lazily ----
  # r <- terra::rast(tif_files)
}


```


```{r}
tif_files = read_glc_v2(beta_test = TRUE)
```

Next incorporate:
```{r}
library(terra)

```

```{r}
tif_files
```
```{r}
prepare_glc_layers <- function(tif_files) {
  library(terra)
  library(lubridate)

  # ---- Step 1: Load and rename rasters by year range ----
  ras_list <- vector("list", length(tif_files))
  
  for (i in seq_along(tif_files)) {
    r_i <- rast(tif_files[i])
    fname <- basename(tif_files[i])
    
    # Parse start and end years from filename
    start_year <- as.numeric(sub(".*_(\\d{4})\\d{4}.*", "\\1", fname))
    end_year   <- as.numeric(sub(".*_(\\d{8}).*", "\\1", fname))
    end_year   <- as.numeric(substr(end_year, 5, 8))
    
    # Build the sequence of years
    years <- seq(start_year, end_year)
    
    # Rename layers safely
    names(r_i) <- years[seq_len(min(length(years), nlyr(r_i)))]
    
    ras_list[[i]] <- r_i
  }

  # ---- Step 2: Filter layers by PRIO-GRID years ----
  years_to_keep <- year(pg_dates()) |> unique()
  
  ras_list_filtered <- lapply(ras_list, function(r) {
    raw_names <- names(r)
    layer_years <- suppressWarnings(as.numeric(gsub("\\D", "", raw_names)))
    keep_layers <- layer_years %in% years_to_keep
    
    if (any(keep_layers)) {
      r <- r[[keep_layers]]
      names(r) <- layer_years[keep_layers]
    } else {
      warning("No matching years found for raster: ", sources(r))
      r <- NULL
    }
    return(r)
  })
  
  # Remove NULL rasters
  ras_list_filtered <- Filter(Negate(is.null), ras_list_filtered)
  
  # ---- Step 3: Rename to full YYYY-MM-DD format ----
  month_to_use <- pg_dates() |> month() |> max()
  day_to_use   <- pg_dates() |> day()   |> max()
  
  ras_list_filtered <- lapply(ras_list_filtered, function(r) {
    yrs <- as.numeric(names(r))
    names(r) <- as.Date(paste(yrs, month_to_use, day_to_use, sep = "-"))
    return(r)
  })
  
  # ---- Return processed rasters ----
  message("✅ Layer preparation complete. Returning filtered raster list.")
  return(ras_list_filtered)
}

```


```{r}
ras_list_filtered <- prepare_glc_layers(tif_files)
```


```{r}
# Step 3: Verify results
# --------------------------------------

# 1️⃣ Check how many raster tiles were returned
length(ras_list_filtered)

# 2️⃣ Check the number of layers per raster (should match pg_dates() years)
sapply(ras_list_filtered, nlyr)

# 3️⃣ Inspect the first raster's layer names
head(names(ras_list_filtered[[1]]))

# 4️⃣ Inspect the full date range for the first raster
range(names(ras_list_filtered[[1]]))

# 5️⃣ Optional — quick visual preview of one raster layer
plot(ras_list_filtered[[1]][[1]], main = names(ras_list_filtered[[1]])[1])
```
```{r}
glc_landcover <- function(landcovertype, beta_test = FALSE) {
  memfrac_option <- terra::terraOptions(verbose = FALSE)$memfrac
  terra::terraOptions(memfrac = 0.8)
  
  # Read and prepare rasters directly
  tif_files <- read_glc_v2(beta_test = beta_test)
  ras_list_filtered <- prepare_glc_layers(tif_files)
  
  # Notify about coverage
  message("Running landcover computation on ", length(ras_list_filtered), " tiles.")
  message("Missing or empty tiles will be skipped automatically.")
  
  res <- robust_transformation(ras_list_filtered, function(x) mean(x == landcovertype))
  
  terra::terraOptions(memfrac = memfrac_option)
  return(res)
}
```


```{r}
# --- Cropland ---
gen_glc_cropland <- function(beta_test = FALSE) {
  glc_landcover(
    landcovertype = c(10, 11, 12, 20),
    beta_test = beta_test
  )
}

# --- Forest ---
gen_glc_forest <- function(beta_test = FALSE) {
  glc_landcover(
    landcovertype = c(51, 52, 61, 62, 71, 72, 81, 82, 91, 92),
    beta_test = beta_test
  )
}

# --- Shrubland ---
gen_glc_shrubland <- function(beta_test = FALSE) {
  glc_landcover(
    landcovertype = c(120, 121, 122, 150, 152),
    beta_test = beta_test
  )
}

# --- Grassland ---
gen_glc_grassland <- function(beta_test = FALSE) {
  glc_landcover(
    landcovertype = c(130, 153),
    beta_test = beta_test
  )
}

# --- Wetland ---
gen_glc_wetland <- function(beta_test = FALSE) {
  glc_landcover(
    landcovertype = c(181, 182, 183, 184, 185, 186, 187),
    beta_test = beta_test
  )
}

# --- Built-up / Urban ---
gen_glc_builtup <- function(beta_test = FALSE) {
  glc_landcover(
    landcovertype = 190,
    beta_test = beta_test
  )
  
# --- Water Body ---
gen_glc_water <- function(beta_test = FALSE) {
  glc_landcover(
    landcovertype = 210,
    beta_test = beta_test
  )
}

```





### Sandbox:

```{r}
# Create a list to hold renamed rasters
ras_list <- vector("list", length(tif_files))

for (i in seq_along(tif_files)) {
  # Load one tif lazily
  r_i <- rast(tif_files[i])
  fname <- basename(tif_files[i])
  
  # Parse start and end years from filename
  start_year <- as.numeric(sub(".*_(\\d{4})\\d{4}.*", "\\1", fname))
  end_year   <- as.numeric(sub(".*_(\\d{8}).*", "\\1", fname))
  end_year   <- as.numeric(substr(end_year, 5, 8))
  
  # Build the sequence of years
  years <- seq(start_year, end_year)
  
  # Rename layers safely
  names(r_i) <- years[seq_len(min(length(years), nlyr(r_i)))]
  
  # Store renamed raster in list
  ras_list[[i]] <- r_i
}

# Inspect one example
names(ras_list[[30]])[1:10]

```
## Step 2
```{r}
# Replace your previous filter step with this improved one
years_to_keep <- lubridate::year(pg_dates()) |> unique()

ras_list_filtered <- lapply(ras_list, function(r) {
  # Clean up names BEFORE filtering
  raw_names <- names(r)
  layer_years <- suppressWarnings(as.numeric(gsub("\\D", "", raw_names)))
  
  # Now filter
  keep_layers <- layer_years %in% years_to_keep
  
  if (any(keep_layers)) {
    r <- r[[keep_layers]]
    names(r) <- layer_years[keep_layers]  # overwrite with clean numeric years
  } else {
    warning("No matching years found for raster: ", sources(r))
    r <- NULL
  }
  
  return(r)
})
```

## Step 3
```{r}
# Continue with downstream steps (unchanged)
ras_list_filtered <- Filter(Negate(is.null), ras_list_filtered)

month_to_use <- pg_dates() |> lubridate::month() |> max()
day_to_use   <- pg_dates() |> lubridate::day()   |> max()

# Rename to full date labels (safe)
ras_list_filtered <- lapply(ras_list_filtered, function(r) {
  yrs <- as.numeric(names(r))
  names(r) <- as.Date(paste(yrs, month_to_use, day_to_use, sep = "-"))
  return(r)
})

```

```{r}
# Inspect the layer names from the first raster
head(names(ras_list_filtered[[1]]))

```


```{r}
library(terra)

# Path to your example tif
f <- "/Volumes/T7/PRIOGRID/GLC_FCS30/v2/7f03a296-4329-4458-8b62-83c3d27530af/GLC_FCS30D_19852022maps_E0-E5/GLC_FCS30D_20002022_E0N50_Annual_V1.1.tif"

# Load the file lazily
r <- rast(f)

# Check metadata
r

# Extract start/end years from filename
fname <- basename(f)
start_year <- as.numeric(sub(".*_(\\d{4})\\d{4}.*", "\\1", fname))
end_year   <- as.numeric(sub(".*_(\\d{8}).*", "\\1", fname))
end_year   <- as.numeric(substr(end_year, 5, 8))  # just last 4 digits

# Build sequence of years
years <- seq(start_year, end_year)

# Assign layer names as actual years
names(r) <- years

# Check first few
names(r)[1:23]

```

```{r}
f <- "/Volumes/T7/PRIOGRID/GLC_FCS30/v2/7f03a296-4329-4458-8b62-83c3d27530af/GLC_FCS30D_19852022maps_E0-E5/GLC_FCS30D_20002022_E0N50_Annual_V1.1.tif"
r <- rast(f)

# Extract start/end years from filename
fname <- basename(f)
start_year <- as.numeric(sub(".*_(\\d{4})\\d{4}.*", "\\1", fname))
end_year   <- as.numeric(sub(".*_(\\d{8}).*", "\\1", fname))
end_year   <- as.numeric(substr(end_year, 5, 8))  # just last 4 digits

# Build sequence of years
years <- seq(start_year, end_year)

# Assign layer names as actual years
names(r) <- years

# Check first few
names(r)[1:10]
```

